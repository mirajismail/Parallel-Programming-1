# Q5
Change directory into gameoflife and run:
```
source teachsetup
make clean
make
```

For the integration test, I outputted an example of the serial code, `./gameoflife 30 30 50 0.2 1` into `validation_set.out` and for the script, integration\_test.sh, I compared the output of any run with the same parameters to validation\_set.out.
```
#!/bin/bash

./gameoflife 30 30 50 0.2 1 > current.out

diff validation_set.out current.out > /dev/null

if [ $? -eq 0 ]; then
    echo "OK"
else
    echo "MISMATCH"
    diff validation_set.out current.out | head -n 20
fi
```

To parallelize `update_all_cells()`, I simply needed to add loop parallelization to the one for loop in the function. 

to parallelize `output_alive_cells()`, initially I added loop parallelization to all the loops, and made `alive_list[n++] = i * num_cols + j;` - I quickly realized this does not work when running integration\_test.sh as it doesn't maintain ordering. After struggling with this problem, I consulted the internet for a bit of help, and came across papers on stream compaction in parallel programming. Listing 2 of page 2 of the following paper: https://www.cse.chalmers.se/~uffe/streamcompaction.pdf illustrates the prefix sum algorithm which consists of the following idea:
(1) a counting pass,
(2) a prefix-sum pass, and
(3) a scattering pass where each thread writes its element to its computed position.


Step 1 is already done in the starter code - count alive cells per row. I parallelized this loop using parallel for - nothing too fancy.
When trying to parallelize `alive_list[n++] = i * num_cols + j;`, the problem is n++ is inherently serial. Thus, we must precompute what n should be for each row. This is where the prefix sum method comes in. I changed the initial loop that i mentioned before to contain a vector `row_counts`. row\_counts[i] = how many alive cells are in row i.

We use this because:
- The serial loop processes rows in order.

- Every row contributes a contiguous block of indices into alive\_list.

- If we know how many items each row produces, we can compute where each row’s block begins.

The new step that I've added was calculating the prefix sum, `row_start[i] = row_start[i-1] + row_counts[i-1];`. Here, `row_start[i]` = where row i’s block begins in the final output array.

This allows for the following:

- Row 0 starts at 0

- Row 1 starts right after row 0's block

- Row 2 starts right after row 1's block

etc.

This reproduces exactly the same ordering as the serial n++ loop.

Now the second pass can be parallel as each row has:

- its own starting offset (row\_start[i])

- its own number of values (row\_counts[i])

Each thread writes into its own slice of alive\_list.

`jobs.sh` is a script for the teach cluster that will run the parallelized code with arguments “1000 1000 1000 0.183 1”, from 1 to 16 threads. The output is in `timings.txt`.

```
threads	runtime_sec
1	37.46
2	34.49
3	30.79
4	29.22
5	28.29
6	27.68
7	27.28
8	26.88
9	26.69
10	26.50
11	26.25
12	26.11
13	26.82
14	25.87
15	25.75
16	25.61
```
